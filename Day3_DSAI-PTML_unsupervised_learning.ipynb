{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised learning \n",
    "\n",
    "<p align=\"center\">\n",
    "  <img src=\"https://pbs.twimg.com/media/DsCTvc3XQAE7Njb?format=jpg&name=4096x4096\" width=\"75%\">\n",
    "<p>\n",
    "\n",
    "Unsupervised learning tries to find patterns within the data without pre-assigned labels.\n",
    "For example, apples and bananas are fruits but, they are clearly different regarding their color and shape. Therefore, unsupervised learning is known as _learning without a teacher_.\n",
    "\n",
    "Unsupervised learning can be classified into two categories:\n",
    "- **Parametric Unsupervised Learning**. It assumes that sample data comes from a population that follows a probability distribution based on a fixed set of parameters. One example of this are *Gaussian Mixture Models* (<a href='#section4'>section 4</a>), which assume all the data points are generated from a mixture of a Gaussian distributions with certain mean and covariance.\n",
    "- **Non-parametric Unsupervised Learning**. They don't make prior assumptions on the ditribution of the population. Data is grouped into clusters based on their similarity. This is the case of *k-means* (<a href='#section2'>section 2</a>) and *hierarchical clustering* (<a href='#section3'>section 3</a>).\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from matplotlib import rcParams\n",
    "\n",
    "rcParams[\"font.size\"] = 15\n",
    "rcParams[\"figure.figsize\"] = (8, 8)\n",
    "plt.style.use(\"ggplot\")\n",
    "plt.style.use(\"seaborn-white\")\n",
    "sns.set_style(\"white\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import pairwise_distances\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.cluster.hierarchy import dendrogram"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>\n",
    "## 1. Distance and dissimilarity metrics\n",
    "Clustering can be considered the most important unsupervised learning problem. A cluster is therefore a collection of objects which are “similar” between them and are “dissimilar” to the objects belonging to other clusters.\n",
    "<img src=\"https://miro.medium.com/max/855/0*9ksfYh14C-ARETav.\" width=\"75%\">\n",
    "\n",
    "What constitutes a good clustering? There is no absolute “best” criterion. It is the user who should supply this criterion, in such a way that the result of the clustering will suit their needs.\n",
    "\n",
    "To find a particular clustering solution, we need to define what \"similar/dissimilar\" means. There are various similarity (the higher, the closer) and distance (the higher, the further) metrics which can be used for clustering:\n",
    "- Euclidean distance. The straight-line distance between two vectors.\n",
    "$$\n",
    "d_{euc}(x,y) = \\sqrt{\\sum_{i=1}^n(x_i - y_i)^2}\n",
    "$$\n",
    "- Manhattan distance (aka \"cityblock\"). Distance from one vector to another. You can imagine this metric as a way to compute the distance between two points when you are not able to go through buildings. It may be preferable to Euclidean in the case of high dimensional data.\n",
    "$$\n",
    "d_{man}(x,y) = \\sum_{i=1}^n |{(x_i - y_i)|}\n",
    "$$\n",
    "<img src=\"https://miro.medium.com/max/500/1*8PYcXZ3oyqYkQOXZrl7lRQ.png\" width=\"30%\"> <div align=\"center\"><em>In green, the Euclidean distance. In blue, Manhattan distance. </em></div>\n",
    "\n",
    "- Correlation-based similarity. It will identify clusters of observations with the same overall profiles regardless of their magnitudes (e.g. clusters of genes being up- and down-regulated together, regardless of being highly or lowly expressed). Correlation coefficients ($R$) are converted to a \"distance-like\" metric by doing $d = 1-R$.\n",
    "    - Pearson’s correlation coefficient is a measure related to the strength and direction of a linear relationship.\n",
    "$$\n",
    "d_{cor}(x, y) = 1 - \\frac{\\sum\\limits_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y})}{\\sqrt{\\sum\\limits_{i=1}^n(x_i - \\bar{x})^2 \\sum\\limits_{i=1}^n(y_i -\\bar{y})^2}}\n",
    "$$\n",
    "    - Spearman’s correlation is a non-parametric measure of monotonic relationship. It is computed the same as Pearson's but using ranked data values. This makes Spearman’s correlation more robust to outliers.\n",
    "$$\n",
    "d_{spear}(x, y) = 1 - \\frac{\\sum\\limits_{i=1}^n (x'_i - \\bar{x'})(y'_i - \\bar{y'})}{\\sqrt{\\sum\\limits_{i=1}^n(x'_i - \\bar{x'})^2 \\sum\\limits_{i=1}^n(y'_i -\\bar{y'})^2}}\n",
    "$$\n",
    "where $x'_i = rank(x_i)$ and $y'_i = rank(y_i)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "data = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import pairwise_distances\n",
    "#help(pairwise_distances)\n",
    "X = pd.DataFrame(data[\"data\"], columns=data[\"feature_names\"])\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the iris dataset, we compute the four distance matrices for all the four metrics above (Di sini kita coba menghitung: (1) euclidean distance (2) .manhattan distance (3) pearson correlation distance dan (4) spearman correlation distance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "euc = pd.DataFrame(\n",
    "    pairwise_distances(X, metric=\"euclidean\"), index=X.index, columns=X.index\n",
    ")\n",
    "print(euc)\n",
    "\n",
    "# Manhattan distance\n",
    "man = pd.DataFrame(\n",
    "    pairwise_distances(X, metric=\"manhattan\"), index=X.index, columns=X.index\n",
    ")\n",
    "print(man)\n",
    "\n",
    "# Pearson correlation distance\n",
    "pearsond = 1 - X.T.corr(method=\"pearson\")\n",
    "print(pearsond)\n",
    "\n",
    "# Spearman correlation distance\n",
    "spearmand = 1 - X.T.corr(method=\"spearman\")\n",
    "print(spearmand)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We identify the most distance and most similar pairs of items by each of the methods. Kecuali pada diagonalnya (karena merupakan jarak untuk nilainya sendiri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Euclidean distance\n",
    "print(\"Euclidean Distance\")\n",
    "np.fill_diagonal(euc.values, np.nan)\n",
    "print(euc[euc == euc.min().min()].dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\"))\n",
    "print(euc[euc == euc.max().max()].dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\"))\n",
    "\n",
    "# Manhattan distance\n",
    "print(\"Manhattan Distance\")\n",
    "np.fill_diagonal(man.values, np.nan)\n",
    "print(man[man == man.min().min()].dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\"))\n",
    "print(man[man == man.max().max()].dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\"))\n",
    "\n",
    "# Pearson correlation distance\n",
    "print(\"Pearson Distance\")\n",
    "np.fill_diagonal(pearsond.values, np.nan)\n",
    "print(pearsond[pearsond == pearsond.min().min()].dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\"))\n",
    "print(pearsond[pearsond == pearsond.max().max()].dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\"))\n",
    "\n",
    "# Spearman correlation distance\n",
    "print(\"Spearman Distance\")\n",
    "np.fill_diagonal(spearmand.values, np.nan)\n",
    "print(spearmand[spearmand == spearmand.min().min()].dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\"))\n",
    "print(spearmand[spearmand == spearmand.max().max()].dropna(axis=0, how=\"all\").dropna(axis=1, how=\"all\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we compute the average distance of these setosa items to either setosa themselves, versicolor items, or virginica items. Plot a histogram of how far away are each of the three species from the setosa items."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset setosa\n",
    "setosa_euc = euc.loc[data[\"target\"] == 0, :]\n",
    "\n",
    "# Get average distance of setosa against versicolor and virginica\n",
    "df = pd.DataFrame(index=setosa_euc.index)\n",
    "df[\"setosa\"] = setosa_euc.loc[:, data[\"target\"] == 0].mean(axis=1)\n",
    "df[\"versicolor\"] = setosa_euc.loc[:, data[\"target\"] == 1].mean(axis=1)\n",
    "df[\"virginica\"] = setosa_euc.loc[:, data[\"target\"] == 2].mean(axis=1)\n",
    "\n",
    "# Plot\n",
    "df.plot.hist(bins=20, alpha=0.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>\n",
    "## 2. k-means clustering\n",
    "\n",
    "K-means clustering is one of the most popular clustering algorithms for its simplicity and broad applicability. It tries to cluster the samples into *k* pre-defined groups using all the features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lloyd's Algorithm (a.k.a. naive k-means)\n",
    "\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/e/ea/K-means_convergence.gif\" width=\"25%\">\n",
    "\n",
    "\n",
    "Lloyd's algorithm is one of the first implementations of k-means clustering that consists of:\n",
    "\n",
    "1. **Initialization**:\n",
    "    - Pre-define **k** groups (clusters) of observations.\n",
    "    - Pre-define **k** centroids sampling randomly from the observations.\n",
    "2. **Assignment**: Assign every observation to a cluster initializing centroids.\n",
    "3. **Update**: Calculate the centroid (mean) across observations within the cluster.\n",
    "4. Go back to 1, until the cluster assignments don't change or we reach a maximum number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample data\n",
    "X = pd.DataFrame(data[\"data\"], columns=data[\"feature_names\"])\n",
    "X[\"species\"] = data[\"target\"]\n",
    "print(X.shape)\n",
    "\n",
    "sns.pairplot(X.astype({\"species\": \"category\"}), hue=\"species\", plot_kws={\"alpha\": 0.5})\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see below, the iris dataset comprises information on 3 different plant species. Apply `sklearn.cluster.KMeans` to the iris dataset to check it.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "#help(KMeans)\n",
    "model = KMeans(3)\n",
    "labels = model.fit_predict(X)\n",
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Try using `sns.pairplot` to visualize how the algorithm behaved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run our kmeans\n",
    "X[\"labels\"] = labels\n",
    "\n",
    "# visualize\n",
    "sns.pairplot(\n",
    "    X.drop(columns=\"species\").astype({\"labels\": \"category\"}),\n",
    "    hue=\"labels\",\n",
    "    plot_kws={\"alpha\": 0.5},\n",
    ")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Count how well species and predicted labels overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = (X.groupby([\"species\", \"labels\"])\n",
    "      .size()\n",
    "      .reset_index(name=\"count\")\n",
    "      .pivot(index=\"labels\", columns=\"species\", values=\"count\"))\n",
    "\n",
    "g = sns.heatmap(df, cmap=\"coolwarm\", annot=True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>\n",
    "## 3. Hierarchical clustering\n",
    "Hierarchical clustering can generally be divided in two types:\n",
    "- **Agglomerative**: \"bottom-up\", each observation starts in its own cluster, and pairs of clusters are merged as one moves up the hierarchy.\n",
    "- **Divisive**: \"top-down\", all observations start in one cluster, and splits are performed recursively as one moves down the hierarchy.\n",
    "\n",
    "In this session we will focus on the more popular agglomerative approach. Given a set of N items to be clustered, and an N*N distance (or similarity) matrix, this is the basic process of agglomerative hierarchical clustering:\n",
    "\n",
    "1. Start by assigning each item to a cluster, so that if you have N items, you now have N clusters, each containing just one item.\n",
    "2. Find the closest (most similar) pair of clusters and merge them into a single cluster, so that now you have one cluster less.\n",
    "3. Compute distances between the new cluster and each of the old clusters.\n",
    "4. Repeat steps 2 and 3 until all items are clustered into a single cluster of size N.\n",
    "\n",
    "<img src=\"https://dashee87.github.io/images/hierarch.gif\" width=\"75%\">\n",
    "    \n",
    "### Linkage criteria\n",
    "The key element in this process is how the similarity/distance between two clusters is computed (step 3 above), the so-called *linkage*. Some commonly used linkage criteria are:\n",
    "- **Maximum or complete-linkage clustering**: it computes the distance between two clusters as the distance between the two points in each cluster that are furthest together. It results in a large number of well-balanced compact clusters, sometimes keeping similar observations in separate clusters.\n",
    "\n",
    "$$\n",
    "d_{AB} = \\mbox{max}_{i \\in A,j \\in B} d_{ij}\n",
    "$$\n",
    "\n",
    "\n",
    "- **Minimum or single-linkage clustering**: it computes the distance between two clusters as the distance between the two points in each cluster that are closest together. It results in a few clusters consisting of long extensive chains of observations as well as several singletons (observations that form their own cluster, i.e. outliers). Disparate clusters may be joined when they have two close points, but are otherwise far apart.\n",
    "\n",
    "$$\n",
    "d_{AB} = \\mbox{min}_{i \\in A,j \\in B} d_{ij}\n",
    "$$\n",
    "\n",
    "\n",
    "- **Unweighted Pair Group Method with Arithmetic Mean (or UPGMA)**: average of all pairwise dissimilarities between observations $i$ in cluster $A$ and $j$ in cluster $B$, where $n$ is the number of observations in each cluster. It is a compromise between complete-linkage and single-linkage.\n",
    "\n",
    "$$\n",
    "d_{AB} = \\frac{\\sum_{i \\in A} \\sum_{j \\in B} d_{ij}}{n_A.n_B}\n",
    "$$\n",
    "\n",
    "\n",
    "- **Ward's minimum variance method**: Ward's method only works with Euclidean distances and it aims at minimizing the total within-cluster variance (i.e. get the most compact clusters possible). Therefore, at each step 2 above, it finds the pair of clusters that leads to minimum increase in total within-cluster variance (aka *sum of squares*) after merging. As a result, it leads to nicely balanced clusters.\n",
    "\n",
    "\n",
    "<img src=\"https://dashee87.github.io/images/hierarch_1.gif\" width=\"75%\">\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "X = pd.DataFrame(data[\"data\"], columns=data[\"feature_names\"])\n",
    "X.head()\n",
    "\n",
    "# https://scikit-learn.org/stable/auto_examples/cluster/plot_agglomerative_dendrogram.html\n",
    "def plot_dendrogram(model, **kwargs):\n",
    "    # Create linkage matrix and then plot the dendrogram\n",
    "\n",
    "    # create the counts of samples under each node\n",
    "    counts = np.zeros(model.children_.shape[0])\n",
    "    n_samples = len(model.labels_)\n",
    "    for i, merge in enumerate(model.children_):\n",
    "        current_count = 0\n",
    "        for child_idx in merge:\n",
    "            if child_idx < n_samples:\n",
    "                current_count += 1  # leaf node\n",
    "            else:\n",
    "                current_count += counts[child_idx - n_samples]\n",
    "        counts[i] = current_count\n",
    "\n",
    "    linkage_matrix = np.column_stack(\n",
    "        [model.children_, model.distances_, counts]\n",
    "    ).astype(float)\n",
    "    # Plot the corresponding dendrogram\n",
    "    dendrogram(linkage_matrix, **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the default options of the `AgglomerativeClustering` function, run the hierarchical algorithm. Plot using the function above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute clustering\n",
    "clustering = AgglomerativeClustering(compute_distances=True).fit(X)\n",
    "\n",
    "# Plot\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "\n",
    "# plot the top three levels of the dendrogram\n",
    "plot_dendrogram(clustering, no_labels=True)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The default `AgglomerativeClustering` uses euclidean distances and Ward's minimum variance method, and takes as an input the data matrix (`X`). However, any custom distance metric can be inputted using the `affinity` parameter. Also, the linkage can be customized using the `linkage` parameter. Take a look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#help(AgglomerativeClustering)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Pearson correlation as the distance metric, compute the agglomerative clusering with UPGMA linkage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute distances\n",
    "pearsond = 1 - X.T.corr(method=\"pearson\")\n",
    "\n",
    "# Compute clustering\n",
    "clustering = AgglomerativeClustering(\n",
    "    compute_distances=True, linkage=\"average\", affinity=\"precomputed\"\n",
    ").fit(pearsond)\n",
    "\n",
    "# Plot\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "\n",
    "# plot the top three levels of the dendrogram\n",
    "plot_dendrogram(clustering, no_labels=True)\n",
    "plt.xlabel(\"Number of points in node (or index of point if no parenthesis).\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using euclidean distances, try out the 4 possible linkages described above. Build the respective dendograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axs = plt.subplots(2, 2)\n",
    "plt.subplot(2, 2, 1)\n",
    "clustering = AgglomerativeClustering(compute_distances=True, linkage=\"ward\").fit(X)\n",
    "plot_dendrogram(clustering, no_labels=True)\n",
    "plt.title(\"Ward's\")\n",
    "\n",
    "plt.subplot(2, 2, 2)\n",
    "clustering = AgglomerativeClustering(compute_distances=True, linkage=\"average\").fit(X)\n",
    "plot_dendrogram(clustering, no_labels=True)\n",
    "plt.title(\"UPGMA\")\n",
    "\n",
    "plt.subplot(2, 2, 3)\n",
    "clustering = AgglomerativeClustering(compute_distances=True, linkage=\"complete\").fit(X)\n",
    "plot_dendrogram(clustering, no_labels=True)\n",
    "plt.title(\"Complete\")\n",
    "\n",
    "plt.subplot(2, 2, 4)\n",
    "clustering = AgglomerativeClustering(compute_distances=True, linkage=\"single\").fit(X)\n",
    "plot_dendrogram(clustering, no_labels=True)\n",
    "plt.title(\"Single\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cutting the dendogram at number of clusters 3, compare how did each of the linkages perform in order to classify the real 3 species of the iris dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = X.copy()\n",
    "# Compute clusterings\n",
    "Y[\"ward\"] = AgglomerativeClustering(linkage=\"ward\", n_clusters=3).fit_predict(X)\n",
    "Y[\"single\"] = AgglomerativeClustering(linkage=\"single\", n_clusters=3).fit_predict(X)\n",
    "Y[\"complete\"] = AgglomerativeClustering(linkage=\"complete\", n_clusters=3).fit_predict(X)\n",
    "Y[\"UPGMA\"] = AgglomerativeClustering(linkage=\"average\", n_clusters=3).fit_predict(X)\n",
    "\n",
    "# Add real data\n",
    "Y[\"species\"] = data[\"target\"]\n",
    "# Plot\n",
    "fig, axs = plt.subplots(2, 2)\n",
    "for n, l in enumerate([\"ward\", \"single\", \"complete\", \"UPGMA\"]):\n",
    "    df = (\n",
    "        Y.groupby([\"species\", l])\n",
    "        .size()\n",
    "        .reset_index(name=\"count\")\n",
    "        .pivot(index=l, columns=\"species\", values=\"count\")\n",
    "    )\n",
    "    plt.subplot(2, 2, n + 1)\n",
    "    sns.heatmap(df, cmap=\"coolwarm\", annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>\n",
    "## 4. Clustering with Gaussian Mixture Models\n",
    "\n",
    "Gaussian Mixture Models are a type of probabilitic models that try to split the population into *k* pre-defined groups or components based on multivariate normal distributions.\n",
    "\n",
    "Basicallly, we aim to fit *k* multivariate normal distributions to the data and classify the observations based on their probability of belonging to the different groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Multivariate Normal Distribution\n",
    "\n",
    "$$X = (X_1, X_2, ..., X_k)^T$$\n",
    "$$X \\sim N(\\mu, \\Sigma)$$\n",
    "$$\\mu = E[X] = (E[X_1], E[X_2], ..., E[X_k])^T$$\n",
    "$$\\Sigma_{ij} = E[(X_i - \\mu_i)(X_j - \\mu_j)] = Cov[X_i, X_j]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Estimating the parameters of the Multivariate Normal distributions\n",
    "\n",
    "One of the most popular algorithms to determine the parameters of a mixture of multivariate normal distributions is the Expectation-Maximization algorithm to perform maximum likelihood estimation of the parameters.\n",
    "\n",
    "At the beginning, we know that there are *k* groups of samples. And we assume that each group is characterized by a different multivariate normal distribution.\n",
    "\n",
    "#### Expectation-Maximization algorithm\n",
    "Although we don't want to go to much into details, the basic steps to estimate the parameters of the multivariate normal distributions are:\n",
    "\n",
    "0. **Initialization**: randomly initialize the parameters of *k* multivariate normal distributions and a vector of mixing coefficients\n",
    "1. **E-step**: compute the probability of every observation of belonging to every distribution.\n",
    "2. **M-step**: update the mixing coefficients and the parameters of the distributions\n",
    "3. Iterate until convergence of the parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture\n",
    "\n",
    "# sample data\n",
    "X = pd.DataFrame(data[\"data\"], columns=data[\"feature_names\"])\n",
    "X[\"species\"] = data[\"target\"]\n",
    "\n",
    "gm = GaussianMixture(n_components=3)\n",
    "labels = gm.fit_predict(X)\n",
    "\n",
    "df = X.assign(cluster=labels)\n",
    "sns.scatterplot(data=df, x=\"sepal length (cm)\", y=\"petal length (cm)\", hue=\"cluster\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compare (Gaussian Mixture and K-Means) the clusters obtained with `sklearn.cluster.KMeans`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gm = GaussianMixture(n_components=3)\n",
    "labels_gaussian = gm.fit_predict(X)\n",
    "\n",
    "kmeans = KMeans(3)\n",
    "labels_kmeans = kmeans.fit_predict(X)\n",
    "\n",
    "plt.figure(figsize=(10, 5))\n",
    "\n",
    "plt.subplot(121)\n",
    "sns.scatterplot(\n",
    "    data=X.assign(cluster=labels_gaussian),\n",
    "    x=\"sepal length (cm)\",\n",
    "    y=\"petal length (cm)\",\n",
    "    hue=\"cluster\",\n",
    ")\n",
    "plt.title(\"Gaussian M.M.\")\n",
    "\n",
    "plt.subplot(122)\n",
    "sns.scatterplot(\n",
    "    data=X.assign(cluster=labels_kmeans),\n",
    "    x=\"sepal length (cm)\",\n",
    "    y=\"petal length (cm)\",\n",
    "    hue=\"cluster\",\n",
    ")\n",
    "plt.title(\"K-means\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>\n",
    "## 5. Finding the right \"k\"\n",
    "\n",
    "One of the most difficult parts of unsupervised learning is to make an educated guess of how many clusters does our dataset contain.\n",
    "\n",
    "The most popular methods to assess quantitatively how good the *k* groups in which we have split our observations are:\n",
    "- **Elbow**: use a metric to score how well the observations cluster with different *k*s overall, plot the scores and choose the *k* at the elbow of the curve. Metrics:\n",
    "    - Total within-cluster distances to the centroid\n",
    "    - Average silhouette values\n",
    "\n",
    "- **Silhouette**: visualize the silhouette values for each observation across clusters to assess how well they fit within the cluster considering given the observations outside the cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Elbow method with Total Sum of Squared errors\n",
    "\n",
    "Within cluster total sum of squares,\n",
    "$$SSE = \\sum_{p=1}^{N}\\sum_{j=1}^{M} (X_{pj} - c_{k_p,j})^2$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Slihouette method\n",
    "- measures how similar a point is to its own cluster (cohesion) compared to other clusters (separation)\n",
    "- ranges from $[-1, 1]$\n",
    "- the higher the better\n",
    "- scores if a point is placed to the right cluster\n",
    "\n",
    "**Silhouette Value**\n",
    "$$\n",
    "    s(i)= \n",
    "\\begin{cases}\n",
    "    \\frac{b(i) - a(i)}{max\\{a(i),b(i)\\}},& \\text{if } |C_i| > 1\\\\\n",
    "    0,              & \\text{if } |C_i| = 1\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where, $a(i)$ is the within cluster average distance for each data point $i$ in cluster $C_i$,\n",
    "$$a(i) = \\frac{1}{|C_i| - 1} \\sum_{j \\in C_i, i \\neq j} d(i,j)$$\n",
    "\n",
    "And, $b(i)$ is the minimum distance of each data point $i$ in cluster $C_i$ with the points in cluster $C_j$:\n",
    "$$b(i) = min_{i \\neq j} \\frac{1}{|C_j|} \\sum_{j \\in C_j} d(i,j)$$\n",
    "\n",
    "Finally, the overall mean silhouette of all values can be interpreted as the score of the clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can choose K using the sihouette score through the elbow method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exercise data\n",
    "X = data[\"data\"]\n",
    "\n",
    "# sum squared errors\n",
    "def SSE(x):\n",
    "    centroid = np.mean(x, axis=0).reshape(1, -1)\n",
    "    sse = (x - centroid) ** 2\n",
    "    sse = np.sum(sse)\n",
    "    return sse\n",
    "\n",
    "\n",
    "SSE([1, 2, 3, 4])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the elbow method to choose the best *k* to cluster the iris dataset with KMeans "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run kmeans for different keys\n",
    "keys = np.arange(2, 10)\n",
    "labels_bykey = {k: KMeans(k).fit_predict(X) for k in keys}\n",
    "\n",
    "# for every k, measure the average squared error within each cluster\n",
    "errors = []\n",
    "for k in keys:\n",
    "    labels = labels_bykey[k]\n",
    "    error = 0\n",
    "    for label in np.unique(labels):\n",
    "        X_incluster = X[labels == label, :]\n",
    "\n",
    "        # transform into error\n",
    "        error += SSE(X_incluster)\n",
    "\n",
    "    # save total error sum\n",
    "    errors.append(error)\n",
    "\n",
    "sns.lineplot(x=keys, y=errors, marker=\"o\", linestyle=\"dashed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the same result with a Gaussian Mixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.mixture import GaussianMixture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cluster for different keys\n",
    "keys = np.arange(2, 10)\n",
    "labels_bykey = {k: GaussianMixture(k).fit_predict(X) for k in keys}\n",
    "\n",
    "# for every k, measure the average squared error within each cluster\n",
    "errors = []\n",
    "for k in keys:\n",
    "    labels = labels_bykey[k]\n",
    "    error = 0\n",
    "    for label in np.unique(labels):\n",
    "        X_incluster = X[labels == label, :]\n",
    "\n",
    "        # transform into error\n",
    "        error += SSE(X_incluster)\n",
    "\n",
    "    # save total error sum\n",
    "    errors.append(error)\n",
    "\n",
    "sns.lineplot(x=keys, y=errors, marker=\"o\", linestyle=\"dashed\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Cluster the iris dataset with your favourite algorithm and apply the silhouette method through `sklearn.metrics.silhouette_score` and `sklearn.metrics.silhouette_samples` for one *k*. Try to visualize the result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import silhouette_score, silhouette_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute KMeans with random_state=123\n",
    "labels = KMeans(3, random_state=123).fit_predict(X)\n",
    "\n",
    "# get silhouette scores for every sample and overall mean\n",
    "sil_samples = silhouette_samples(X, labels)\n",
    "sil_score = silhouette_score(X, labels)\n",
    "\n",
    "df = (\n",
    "    pd.DataFrame({\"labels\": labels, \"silhouette\": sil_samples})\n",
    "    .sort_values([\"labels\", \"silhouette\"])\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .astype({\"labels\": \"category\"})\n",
    ")\n",
    "\n",
    "sns.scatterplot(x=\"index\", y=\"silhouette\", hue=\"labels\", data=df)\n",
    "plt.axhline(y=sil_score, linestyle=\"dashed\", color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Which *k* should we use based on the average silhouette score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use silhouette_score() for different ks\n",
    "ks = np.arange(2, 10)\n",
    "sil = np.array([silhouette_score(X, KMeans(k).fit_predict(X)) for k in ks])\n",
    "\n",
    "# visualize the result\n",
    "sns.lineplot(x=ks, y=sil, marker=\"o\", linestyle=\"dashed\")\n",
    "plt.show()\n",
    "\n",
    "# check out the silhouettes by sample using the new k\n",
    "labels = KMeans(2, random_state=123).fit_predict(X)\n",
    "sil = silhouette_samples(X, labels)\n",
    "\n",
    "df = (\n",
    "    pd.DataFrame({\"labels\": labels, \"silhouette\": sil})\n",
    "    .sort_values([\"labels\", \"silhouette\"])\n",
    "    .reset_index(drop=True)\n",
    "    .reset_index()\n",
    "    .astype({\"labels\": \"category\"})\n",
    ")\n",
    "\n",
    "sns.scatterplot(x=\"index\", y=\"silhouette\", hue=\"labels\", data=df)\n",
    "plt.axhline(y=np.mean(sil), linestyle=\"dashed\", color=\"black\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## REFERENCES\n",
    "- _Unsupervised Learning and Data Clustering_. Sanatan Mishra. https://towardsdatascience.com/unsupervised-learning-and-data-clustering-eeecb78b422a\n",
    "- _Clustering with Scikit with GIFs_. David Sheehan. https://dashee87.github.io/data%20science/general/Clustering-with-Scikit-with-GIFs/\n",
    "- _Calculate Similarity — the most relevant Metrics in a Nutshell_. Marvin Lüthe. https://towardsdatascience.com/calculate-similarity-the-most-relevant-metrics-in-a-nutshell-9a43564f533e\n",
    "- *How to Determine the Optimal K for K-Means?*. Khyati Mahendru.\n",
    "https://medium.com/analytics-vidhya/how-to-determine-the-optimal-k-for-k-means-708505d204eb\n",
    "- *K-means clustering*. Sherry Towers.\n",
    "http://sherrytowers.com/2013/10/24/k-means-clustering/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "toc-autonumbering": false
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
